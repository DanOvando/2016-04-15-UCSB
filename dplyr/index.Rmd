---
title: "Data wrangling in R"
author: "Ben Best and Julie Lowndes"
date: "April 16, 2016"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information. - [NYTimes (2014)](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)

Today's materials are again borrowing from some excellent sources, including

- Dr. Jenny Bryan's lectures from STAT545 at UBC: [Introduction to dplyr](http://stat545.com/block009_dplyr-intro.html)
- Software Carpentry's R for reproducible scientific analysis materials: [Dataframe manipulation with dplyr](http://swcarpentry.github.io/r-novice-gapminder/13-dplyr.html)

Today we're going to learn about a package by Hadley Wickham called `dplyr` and how it will help you with simple data exploration, and how you can use it in combination with the `%>%` operator for more complex wrangling (including a lot of the things you would use for loops for. 

# install our first package: `dplyr`

Packages are bundles of functions, along with help pages and other goodies that make them easier for others to use, (ie. vignettes). 

So far we've been using packages included in 'base R'; they are 'out-of-the-box' functions. You can also install packages from online. The most traditional is [CRAN, the Comprehensive R Archive Network](https://cran.r-project.org/). This is where you went to download R originally, and will go again to look for updates. 

You don't need to go to CRAN's website to install packages, we can do it from within R with the command `install.packages("package-name-in-quotes")`.
```{r}
## from CRAN:
#install.packages("dplyr") ## do this once only to install the package on your computer.

library(dplyr) ## do this every time you restart R and need it 
```
What's the difference between `install.packages()` and `library()`? Here's my analogy: 

- `install.packages()` is setting up electricity for your house. Just need to do this once (let's ignore monthly bills). 
- `library()` is turning on the lights. You only turn them on when you need them, otherwise it wouldn't be efficient. And when you quit R, and come back, you'll have to turn them on again with `library()`, but you already have your electricity set up.

# Use `dplyr::filter()` to subset data row-wise.

First let's read in the gapminder data. 
```{r}
gapminder <- read.csv('data/gapminder-FiveYearData.csv')
```

`filter()` takes logical expressions and returns the rows for which all are `TRUE`. Visually, we are doing this (thanks RStudio for your [cheatsheet](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)): 

![](fig/rstudio-cheatsheet-filter.png)

```{r eval=FALSE}
filter(gapminder, lifeExp < 29)
filter(gapminder, country == "Rwanda")
filter(gapminder, country %in% c("Rwanda", "Afghanistan"))
```

Compare with some base R code to accomplish the same things
```{r eval = FALSE}
gapminder[gapminder$lifeExp < 29, ] ## repeat `gapminder`, [i, j] indexing is distracting
subset(gapminder, country == "Rwanda") ## almost same as filter ... but wait ...
```

# Meet the new pipe operator

Before we go any further, we should exploit the new pipe operator that `dplyr` imports from the [`magrittr`](https://github.com/smbache/magrittr) package by Stefan Bache. **This is going to change your data analytical life**. You no longer need to enact multi-operation commands by nesting them inside each other. This new syntax leads to code that is much easier to write and to read.

Here's what it looks like: `%>%`. The RStudio keyboard shortcut: Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac).

Let's demo then I'll explain:
```{r eval=FALSE}
gapminder %>% head
```

This is equivalent to `head(gapminder)`. This pipe operator takes the thing on the left-hand-side and __pipes__ it into the function call on the right-hand-side -- literally, drops it in as the first argument.

Never fear, you can still specify other arguments to this function! To see the first 3 rows of Gapminder, we could say `head(gapminder, 3)` or this:
```{r eval=FALSE}
gapminder %>% head(3)
```

**I've advised you to think "gets" whenever you see the assignment operator, `<-`. Similary, you should think "then" whenever you see the pipe operator, `%>%`.**

You are probably not impressed yet, but the magic will soon happen.

### Use `dplyr::select()` to subset the data on variables or columns.

Back to `dplyr` ...

Use `select()` to subset the data on variables or columns. Visually, we are doing this (thanks RStudio for your [cheatsheet](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)): 

![](fig/rstudio-cheatsheet-select.png)

Here's a conventional call:

```{r eval=FALSE}
select(gapminder, year, lifeExp) 
```

But using what we just learned, with a pipe, we can do this:
```{r eval=FALSE}
gapminder %>% select(year, lifeExp)
```

Let's write it again but using multiple lines so it's nicer to read. And let's add a second pipe operator to pipe through `head`:
```{r}
gapminder %>%
  select(year, lifeExp) %>%
  head(4)
```
Think: "Take `gapminder`, then select the variables year and lifeExp, then show the first 4 rows."

# Revel in the convenience
Let's do a little analysis where we calculate the mean gdp for Cambodia. 

Here's the gapminder data for Cambodia, but only certain variables:
```{r eval=FALSE}
gapminder %>%
  filter(country == "Cambodia") %>%
  # select(country, year, pop, gdpPercap) ## entering 4 of the 6 columns is tedious
  select(-continent, -lifeExp) # you can use - to deselect columns
```

and what a typical base R call would look like:

```{r}
gapminder[gapminder$country == "Cambodia", c("country", "year", "pop", "gdpPercap")]
```

or, possibly?, a nicer look using base R's `subset()` function:

```{r}
subset(gapminder, country == "Cambodia", select = c(country, year, pop, gdpPercap))
```

# Use `mutate()` to add new variables

Imagine we wanted to recover each country's GDP. After all, the Gapminder data has a variable for population and GDP per capita. Let's add a new column and multiply them together.

Visually, we are doing this (thanks RStudio for your [cheatsheet](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)): 

![](fig/rstudio-cheatsheet-mutate.png)

```{r eval=FALSE}
gapminder %>%
  mutate(gdp = pop * gdpPercap)
```

> Exercise: how would you add that to the previous `filter` and `select` commands we did with Cambodia:
```{r eval=FALSE}
gapminder %>%
  filter(country == "Cambodia") %>%
  select(-continent, -lifeExp)
```

Answer: 
```{r eval=FALSE}
gapminder %>%
  filter(country == "Cambodia") %>%
  select(-continent, -lifeExp) %>%
  mutate(gdp = pop * gdpPercap)
```

# `group_by` and `summarize`
Great! And now we want to calculate the mean gdp across all years (Let's pretend that's a good idea statistically) 

Visually, we are doing this (thanks RStudio for your [cheatsheet](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)): 
 
![](fig/rstudio-cheatsheet-summarise.png)

```{r eval=FALSE}
gapminder %>%
  filter(country == "Cambodia") %>%
  select(-continent, -lifeExp) %>%
  mutate(gdp = pop * gdpPercap) %>%
  group_by(country) %>%
  summarize(mean_gdp = mean(gdp)) %>%
  ungroup() # if you use group_by, also use ungroup() to save heartache later
```

# Remember our for loop?

And how would you then do this for every country, not just Cambodia? Well, yesterday we would have been thinking about putting this whole analysis inside a for loop, replacing "Cambodia" with a new name each time we iterated through the loop. But today, we have it already, just need to *delete* one line from our analysis--we don't need to filter out Cambodia anymore!! 

Visually, we are doing this (thanks RStudio for your [cheatsheet](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)): 
 
![](fig/rstudio-cheatsheet-group_by.png)

```{r eval=FALSE}
gapminder %>%
  select(-continent, -lifeExp) %>%
  mutate(gdp = pop * gdpPercap) %>%
  group_by(country) %>%
  summarize(mean_gdp = mean(gdp)) %>%
  ungroup() # if you use group_by, also use ungroup() to save heartache later
```

So we have done a pretty incredible amount of work in a few lines. Our whole analysis is this. Imagine the possibilities from here. 

```{r eval=FALSE}
library(dplyr)

gapminder %>% 
  read.csv('data/gapminder-FiveYearData.csv') %>% 
  select(-continent, -lifeExp) %>%
  mutate(gdp = pop * gdpPercap) %>%
  group_by(country) %>%
  summarize(mean_gdp = mean(gdp)) %>%
  ungroup() # if you use group_by, also use ungroup() to save heartache later
```

------

# Rationale 

When performing data analysis in R, code can become quite messy, making it hard to revisit and determine the sequence of operations. Commenting helps. Good variable names help. Still, at least two common issues make code difficult to understand: **multiple variables** and **nested functions**. Let's examine these issues by approaching an analysis presenting both problems, and finally see how `dplyr` offers an elegant alternative.

For example, let's ask of the `surveys.csv` dataset: _**How many observations of species 'NL' appear each year?**_

[TODO: change to gapminder example.]

## Pseudocode

You can write the logic out as **pseudocode** which can become later comments for the actual code:

```{r pseudocode, eval=F}
# read in csv
# view data
# limit columns to species and year
# limit rows to just species "NL"
# get count per year
# write out csv
```

### Multiple Variables

Now let's approach this code sequentially using base functions, ie natively loaded functions in R without need for additional libraries.

```{r multiple variables, eval=F}
# read in csv
surveys = read.csv('../data/r-ecology/surveys.csv') 

# view data
head(surveys)
summary(surveys)

# limit columns to species and year
surveys_2 = surveys[,c('species_id', 'year')]

# limit rows to just species "NL"
surveys_3 = surveys_2[surveys_2$species_id  == 'NL',]

# get count per year
surveys_4 = aggregate(species_id ~ year, data=surveys_3, FUN='length')

# write to csv
write.csv(surveys_4, 'data/surveys_bbest.csv', row.names = FALSE)
```

Because the variables are named sequentially, ie `surveys_2` to `surveys_4`, it is relatively easy to follow, but so often in the course of playing with data these names are very different. And then we quickly lose track of which operations get applied to which variables.

Even with obvious variable names, there is a redunancy, as we'll see shortly, to assigning a new variable name to the output of each operation and input of each subsequent operation.

### Nested Functions

Another common programming trick to reduce variable naming space is to nest the output of one function as the input of the next one. 

```{r nested functions, eval=F}
# read in data
surveys = read.csv('../data/r-ecology/surveys.csv') 

# view data
head(surveys)
summary(surveys)

# limit data with [], aggregate to count, write to csv
write.csv(
  aggregate(
    species_id ~ year, 
    data = surveys[surveys_2$species_id  == 'NL', c('species_id', 'year')], 
    FUN = 'length'), 
  'data/surveys_bbest.csv',
  row.names = FALSE)
```

So the code started the same, and continues using the same functions, but these functions get applied from the input arguments to the outer containing functions, ie in a nested manner: 

1. surveys gets sliced `[]` into rows and columns in one call, which gets used as the `data = ` argument to 

1. `aggregate()`, which applies the `length()` function to get a count to the formula `species_id ~ year` in which the `species_id` gets split into groups based on `year`, which gets further applied as the unnamed first argument to 

1. `write.csv()` which has the additional unnamed argument specifying the output file and named argument turning off the default option to prefix row numbers.

Although we've saved space from not performing the extra naming of variables, we've made the code very difficult to read, needing to parse which functions are arguments to subsequent functions. The indentation helps readability a bit, but now let's examine a far better solution to either approaches above with `dplyr`.

### Elegance of `dplyr` & `%>%`

Next, we'll use the libraries `readr`for improved versions of reading and writing csv files, and `dplyr` for advanced data frame manipulation. Most importantly, `dplyr` uses the "then" operator `%>%` which transfers the output on the left to the first argument of the function on the right. Most simply `surveys %>% summary()` transfers the surveys data frame into the first argument of the summary function. Use of this chaining operator seems excessive in this simple example, but is powerful when chaining together many operations on the same data frame. We're able to efficiently write out operations, get past the previous problem of multiple variable names without the obfuscation of nesting.

```{r dplyr elegance, eval=F}
# load libraries
library(readr)
library(dplyr)
library(magrittr) # for %T>%

# read in csv
surveys = read_csv('../data/r-ecology/surveys.csv') 

# dplyr elegance
surveys %T>%                          # note tee operator %T>% for glimpse
  glimpse() %>%                       # view data
  select(species_id, year) %>%        # limit columns
  filter(species_id  == 'NL') %>%     # limit rows
  group_by(year) %>%                  # get count by first grouping
  summarize(n = n()) %>%              #   then summarize
  write_csv('surveys_summary.csv')    # write out csv
```

Now we can read from the top, starting with the data frame surveys, to see a very clear sequence of operations: 

1. `glimpse()`
1. `select()`
1. `filter()`
1. `group_by()`
1. `summarize()`
1. `write_csv()`

Arguments are minimal without repeating the name of the data frame, or even needing quotations in the case of column names.

The "tee" operator `%T>%` is similar to the "then" operator `%>%` in that the left side is passed to the right, but is then also teed off as the output of the right side. This is useful in this case for `glimpse` since its output is simply printed to the Console and does not otherwise return the data frame needed to continue the sequence of operations. So the "tee" operator `%T>%` is most useful for injecting intermediate operations like printing or plotting that wouldn't otherwise output a return object for continuing operations.

## Summary

The `tidyr` and `dplyr` packages were created by [Hadley Wickham](https://github.com/hadley) of `ggplot2` fame. The "gg" in `ggplot2` stands for the "grammar of graphics". Hadley similarly considers the functionality of the two packages `dplyr` and `tidyr` to provide the "grammar of data manipulation".

Next, we'll explore the data wrangling lessons that [Remi contributed](https://github.com/swcarpentry/r-novice-gapminder/commits?author=remi-daigle) to Software Carpentry.

# `dplyr`

[**dplyr** - Software Carpentry](http://swcarpentry.github.io/r-novice-gapminder/13-dplyr.html)

# `tidyr`

[**tidyr** - Software Carpentry](http://swcarpentry.github.io/r-novice-gapminder/14-tidyr.html)


# Other

* [Tidying up Data - Env Info](http://ucsb-bren.github.io/env-info/wk04_tidyr.html) - [Rmd](https://github.com/ucsb-bren/env-info/blob/gh-pages/wk04_tidyr.Rmd)
* [Data wrangling with dplyr and tidyr - Tyler Clavelle & Dan Ovando](http://bbest.github.io/dplyr-tidyr-tutorial/) - [Rmd](https://github.com/bbest/dplyr-tidyr-tutorial/blob/gh-pages/index.Rmd)
